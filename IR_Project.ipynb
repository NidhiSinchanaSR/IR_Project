{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "4LwLryCa4S6z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import warnings\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Embedding\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MkT1T_N8-b3T"
   },
   "outputs": [],
   "source": [
    "# Read TSV file\n",
    "train_data = pd.read_csv('train_en.tsv', delimiter='\\t')\n",
    "dev_data = pd.read_csv('dev_en.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPZtakBGDMNE",
    "outputId": "754912ca-a627-461a-ee59-e031c5db831f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "text    0\n",
       "HS      0\n",
       "TR      0\n",
       "AG      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the null values in train data\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ak7IpEAoDkLd",
    "outputId": "66a56a34-68c2-4728-82ca-943560b5da99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "text    0\n",
       "HS      0\n",
       "TR      0\n",
       "AG      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the null values in dev data\n",
    "dev_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "V5NYzA2__F9y",
    "outputId": "8bdd6cbc-a7b1-47ac-f3c7-76371f94a350"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  HS  TR AG\n",
       "0  201  Hurray, saving us $$$ in so many ways @potus @...   1   0  0\n",
       "1  202  Why would young fighting age men be the vast m...   1   0  0\n",
       "2  203  @KamalaHarris Illegals Dump their Kids at the ...   1   0  0\n",
       "3  204  NY Times: 'Nearly All White' States Pose 'an A...   0   0  0\n",
       "4  205  Orban in Brussels: European leaders are ignori...   0   0  0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8cN1rWla_QV6",
    "outputId": "f7a010d5-58d6-475c-c5b1-1720c5677376"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18201</td>\n",
       "      <td>I swear I’m getting to places just in the nick...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18202</td>\n",
       "      <td>I’m an immigrant — and Trump is right on immig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18203</td>\n",
       "      <td>#IllegalImmigrants #IllegalAliens #ElectoralSy...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18204</td>\n",
       "      <td>@DRUDGE_REPORT We have our own invasion issues...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18205</td>\n",
       "      <td>Worker Charged With Sexually Molesting Eight C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  HS  TR  AG\n",
       "0  18201  I swear I’m getting to places just in the nick...   0   0   0\n",
       "1  18202  I’m an immigrant — and Trump is right on immig...   0   0   0\n",
       "2  18203  #IllegalImmigrants #IllegalAliens #ElectoralSy...   1   0   1\n",
       "3  18204  @DRUDGE_REPORT We have our own invasion issues...   1   0   1\n",
       "4  18205  Worker Charged With Sexually Molesting Eight C...   0   0   0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one wrong label in AG column of train data\n",
    "train_data['AG'] = train_data['AG'].replace(['discredit'], '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of values of AG column in train data to int\n",
    "train_data[['AG']] = train_data[['AG']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RVbKrPhkCq1M"
   },
   "outputs": [],
   "source": [
    "# Drop the column id\n",
    "# We don't need it\n",
    "\n",
    "train_data = train_data.drop('id', axis=1)\n",
    "dev_data = dev_data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzaAurx3AgzT"
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zC-0AIqy_SmO",
    "outputId": "da51a93d-7660-41f1-8b9b-ee6b7f912eb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chaitanya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/chaitanya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/chaitanya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load NLTK stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load NLTK stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and lowercase the tokens\n",
    "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem and lemmatize the tokens\n",
    "    tokens = [stemmer.stem(lemmatizer.lemmatize(token, pos='v')) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "peJDV-OWBRj0"
   },
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "dev_data['text'] = dev_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DgoppmifB9n4",
    "outputId": "78fd2480-b649-49e1-b934-cdd5cdc2a059"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hurray save us mani way potu realdonaldtrump l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would young fight age men vast major one escap...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kamalaharri illeg dump kid border like road ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ny time nearli white state pose array problem ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orban brussel european leader ignor peopl want...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  HS  TR  AG\n",
       "0  hurray save us mani way potu realdonaldtrump l...   1   0   0\n",
       "1  would young fight age men vast major one escap...   1   0   0\n",
       "2  kamalaharri illeg dump kid border like road ki...   1   0   0\n",
       "3  ny time nearli white state pose array problem ...   0   0   0\n",
       "4  orban brussel european leader ignor peopl want...   0   0   0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data after preprocessing text\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JUej-U0iCE2A",
    "outputId": "28bdeafa-60b4-4433-95b3-f8886ef3262d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>swear get place nick time exhaust sam schulman...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>immigr trump right immigr http co pldngi fmv m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>illegalimmigr illegalalien electoralsystem ele...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drudg report invas issu mexican buildthatwal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worker charg sexual molest eight children immi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  HS  TR  AG\n",
       "0  swear get place nick time exhaust sam schulman...   0   0   0\n",
       "1  immigr trump right immigr http co pldngi fmv m...   0   0   0\n",
       "2  illegalimmigr illegalalien electoralsystem ele...   1   0   1\n",
       "3       drudg report invas issu mexican buildthatwal   1   0   1\n",
       "4  worker charg sexual molest eight children immi...   0   0   0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev data after preprocessing text\n",
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnnqV2fmF2De"
   },
   "source": [
    "### Create vectors of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWsQJgu3Hhb2"
   },
   "source": [
    "#### Tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vsq41VdhEhXC"
   },
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the text data\n",
    "vectorizer.fit(train_data['text'])\n",
    "\n",
    "# Transform the 'text' column\n",
    "train_tfidf_vectors = vectorizer.transform(train_data['text'])\n",
    "dev_tfidf_vectors = vectorizer.transform(dev_data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOa0vMMYHtFE"
   },
   "source": [
    "#### Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "M0-hrphrGnbV"
   },
   "outputs": [],
   "source": [
    "# Convert the preprocessed text into a list of words\n",
    "train_sentences = [text.split() for text in train_data['text']]\n",
    "dev_sentences = [text.split() for text in dev_data['text']]\n",
    "all_sentences = train_sentences + dev_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "44bAp2DFIDMn"
   },
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on the train data\n",
    "train_word2vec_model = Word2Vec(train_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train the Word2Vec model on the dev data\n",
    "dev_word2vec_model = Word2Vec(all_sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "GxzuAS1FKIGb"
   },
   "outputs": [],
   "source": [
    "# train_word2vec_vectors\n",
    "train_word2vec_vectors = []\n",
    "for sentence in train_sentences:\n",
    "    sentence_vector = []\n",
    "    for word in sentence:\n",
    "        if word in train_word2vec_model.wv.key_to_index:\n",
    "            sentence_vector.append(train_word2vec_model.wv[word])\n",
    "    if len(sentence_vector) > 0:\n",
    "        sentence_avg = sum(sentence_vector) / len(sentence_vector)\n",
    "    else:\n",
    "        sentence_avg = np.zeros(train_word2vec_model.vector_size)\n",
    "    train_word2vec_vectors.append(sentence_avg)\n",
    "\n",
    "train_word2vec_vectors = np.array(train_word2vec_vectors)\n",
    "\n",
    "# dev_word2vec_vectors\n",
    "dev_word2vec_vectors = []\n",
    "for sentence in dev_sentences:\n",
    "    sentence_vector = []\n",
    "    for word in sentence:\n",
    "        if word in dev_word2vec_model.wv.key_to_index:\n",
    "            sentence_vector.append(dev_word2vec_model.wv[word])\n",
    "    if len(sentence_vector) > 0:\n",
    "        sentence_avg = sum(sentence_vector) / len(sentence_vector)\n",
    "    else:\n",
    "        sentence_avg = np.zeros(dev_word2vec_model.vector_size)\n",
    "    dev_word2vec_vectors.append(sentence_avg)\n",
    "\n",
    "dev_word2vec_vectors = np.array(dev_word2vec_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woxVyRYWqnmM"
   },
   "source": [
    "#### GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "04r6ftc3X3n1",
    "outputId": "3850081d-4f5a-4397-e627-3046335e978b"
   },
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.300d.txt'\n",
    "glove_vectors = {}\n",
    "\n",
    "with open(glove_file, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "def get_glove_vectors(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in glove_vectors:\n",
    "            vectors.append(glove_vectors[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        return sum(vectors) / len(vectors)\n",
    "\n",
    "train_glove_vectors = []\n",
    "for text in train_data['text']:\n",
    "    train_glove_vectors.append(get_glove_vectors(text))\n",
    "    \n",
    "dev_glove_vectors = []\n",
    "for text in dev_data['text']:\n",
    "    dev_glove_vectors.append(get_glove_vectors(text))\n",
    "    \n",
    "train_glove_vectors = np.array(train_glove_vectors)\n",
    "dev_glove_vectors = np.array(dev_glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYiRJ01y2WRp"
   },
   "source": [
    "#### BERT vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQnPWVPzsIVC",
    "outputId": "8590030a-7d5a-4c19-fd70-357a79022ac6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_vectors(text):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    # Convert the tokens to PyTorch tensors\n",
    "    tokens = torch.tensor([tokens])\n",
    "    # Get the BERT embeddings for the tokens\n",
    "    with torch.no_grad():\n",
    "        embeddings = bert_model(tokens)[0]\n",
    "    # Take the mean of the embeddings to get a single vector for the text\n",
    "    vector = torch.mean(embeddings, dim=1).squeeze()\n",
    "    # Convert the vector to a numpy array\n",
    "    vector = vector.numpy()\n",
    "    return vector\n",
    "\n",
    "train_bert_vectors = []\n",
    "for text in train_data['text']:\n",
    "    train_bert_vectors.append(get_bert_vectors(text))\n",
    "    \n",
    "dev_bert_vectors = []\n",
    "for text in dev_data['text']:\n",
    "    dev_bert_vectors.append(get_bert_vectors(text))\n",
    "    \n",
    "train_bert_vectors = np.array(train_bert_vectors)\n",
    "dev_bert_vectors = np.array(dev_bert_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bert_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels \n",
    "train_output = train_data.loc[:, ['HS', 'TR', 'AG']].values\n",
    "dev_output =  dev_data.loc[:, ['HS', 'TR', 'AG']].values\n",
    "\n",
    "# Types of embeddings\n",
    "vector_types = [\"Tfidf vectors\", \"Word2Vec vectors\", \"GloVe vectors\", \"BERT vectors\"]\n",
    "\n",
    "# Types of Labels\n",
    "labels = ['HS', 'TR', 'AG']\n",
    "\n",
    "train_vectors = {\n",
    "    \"Tfidf vectors\" : train_tfidf_vectors,\n",
    "    \"Word2Vec vectors\" : train_word2vec_vectors,\n",
    "    \"GloVe vectors\" : train_glove_vectors,\n",
    "    \"BERT vectors\" : train_bert_vectors\n",
    "}\n",
    "\n",
    "dev_vectors = {\n",
    "    \"Tfidf vectors\" : dev_tfidf_vectors,\n",
    "    \"Word2Vec vectors\" : dev_word2vec_vectors,\n",
    "    \"GloVe vectors\" : dev_glove_vectors,\n",
    "    \"BERT vectors\" : dev_bert_vectors\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(train_vectors, f)\n",
    "\n",
    "with open('dev_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(dev_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {\n",
    "  'model type' : [],\n",
    "  'vector type' : [],\n",
    "  'HS F1' : [],\n",
    "  'TR F1' : [],\n",
    "  'AG F1' :  [],\n",
    "  'macro-averaged F1' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.6996466431095407\n",
      "TR (F1 score): 0.5380116959064327\n",
      "AG (F1 score): 0.3680555555555556\n",
      "macro-averaged (F1 score): 0.5352379648571763 \n",
      "\n",
      "\n",
      "Model: Logistic Regression\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.2777777777777778\n",
      "TR (F1 score): 0.36500000000000005\n",
      "AG (F1 score): 0.0\n",
      "macro-averaged (F1 score): 0.2142592592592593 \n",
      "\n",
      "\n",
      "Model: Logistic Regression\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.645465253239105\n",
      "TR (F1 score): 0.43692307692307686\n",
      "AG (F1 score): 0.22745098039215686\n",
      "macro-averaged (F1 score): 0.43661310351811294 \n",
      "\n",
      "\n",
      "Model: Logistic Regression\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.6797235023041475\n",
      "TR (F1 score): 0.5978260869565217\n",
      "AG (F1 score): 0.4210526315789474\n",
      "macro-averaged (F1 score): 0.5662007402798722 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # create a logistic regression object\n",
    "  logreg = LogisticRegression(max_iter=1000)\n",
    "  # create a multioutput classifier object\n",
    "  multi_logreg = MultiOutputClassifier(logreg)\n",
    "  # fit the multioutput classifier on training data\n",
    "  multi_logreg.fit(train_vectors[vector_type], train_output)\n",
    "  # predict on test data\n",
    "  y_pred = multi_logreg.predict(dev_vectors[vector_type])\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('Logistic Regression')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: Logistic Regression')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Decision Tree\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.6198156682027651\n",
      "TR (F1 score): 0.567237163814181\n",
      "AG (F1 score): 0.4305555555555555\n",
      "macro-averaged (F1 score): 0.5392027958575005 \n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.5465465465465464\n",
      "TR (F1 score): 0.2929936305732484\n",
      "AG (F1 score): 0.2541176470588235\n",
      "macro-averaged (F1 score): 0.36455260805953943 \n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.5611353711790393\n",
      "TR (F1 score): 0.4267352185089974\n",
      "AG (F1 score): 0.2871046228710462\n",
      "macro-averaged (F1 score): 0.4249917375196943 \n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.579185520361991\n",
      "TR (F1 score): 0.41791044776119396\n",
      "AG (F1 score): 0.35046728971962615\n",
      "macro-averaged (F1 score): 0.4491877526142704 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # create a decision tree classifier object\n",
    "  tree = DecisionTreeClassifier()\n",
    "  # create a multioutput classifier object\n",
    "  multi_tree = MultiOutputClassifier(tree)\n",
    "  # fit the multioutput classifier on training data\n",
    "  multi_tree.fit(train_vectors[vector_type], train_output)\n",
    "  # predict on test data\n",
    "  y_pred = multi_tree.predict(dev_vectors[vector_type])\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('Decision Tree')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: Decision Tree')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.6963855421686748\n",
      "TR (F1 score): 0.5259938837920489\n",
      "AG (F1 score): 0.33802816901408456\n",
      "macro-averaged (F1 score): 0.5201358649916027 \n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.5742411812961443\n",
      "TR (F1 score): 0.345679012345679\n",
      "AG (F1 score): 0.33978132884777124\n",
      "macro-averaged (F1 score): 0.41990050749653146 \n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.6467780429594272\n",
      "TR (F1 score): 0.35738831615120276\n",
      "AG (F1 score): 0.07207207207207207\n",
      "macro-averaged (F1 score): 0.35874614372756736 \n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.6456310679611651\n",
      "TR (F1 score): 0.3745583038869258\n",
      "AG (F1 score): 0.18257261410788386\n",
      "macro-averaged (F1 score): 0.4009206619853249 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # create a random forest classifier object\n",
    "  forest = RandomForestClassifier()\n",
    "  # create a multioutput classifier object\n",
    "  multi_forest = MultiOutputClassifier(forest)\n",
    "  # fit the multioutput classifier on training data\n",
    "  multi_forest.fit(train_vectors[vector_type], train_output)\n",
    "  # predict on test data\n",
    "  y_pred = multi_forest.predict(dev_vectors[vector_type])\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('Random Forest')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: Random Forest')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Support Vector Machine\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.7137809187279152\n",
      "TR (F1 score): 0.5485714285714286\n",
      "AG (F1 score): 0.33935018050541516\n",
      "macro-averaged (F1 score): 0.5339008426015863 \n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.6058394160583942\n",
      "TR (F1 score): 0.0\n",
      "AG (F1 score): 0.0\n",
      "macro-averaged (F1 score): 0.20194647201946472 \n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.6867749419953597\n",
      "TR (F1 score): 0.4466019417475728\n",
      "AG (F1 score): 0.09999999999999999\n",
      "macro-averaged (F1 score): 0.4111256279143109 \n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.6979542719614921\n",
      "TR (F1 score): 0.46349206349206346\n",
      "AG (F1 score): 0.19834710743801653\n",
      "macro-averaged (F1 score): 0.45326448096385735 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # create a SVM classifier object\n",
    "  svm = SVC()\n",
    "  # create a multioutput classifier object\n",
    "  multi_svm = MultiOutputClassifier(svm)\n",
    "  # fit the multioutput classifier on training data\n",
    "  multi_svm.fit(train_vectors[vector_type], train_output)\n",
    "  # predict on test data\n",
    "  y_pred = multi_svm.predict(dev_vectors[vector_type])\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('Support Vector Machine')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: Support Vector Machine')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.6260236578707916\n",
      "TR (F1 score): 0.43478260869565216\n",
      "AG (F1 score): 0.3356401384083045\n",
      "macro-averaged (F1 score): 0.46548213499158275 \n",
      "\n",
      "\n",
      "Model: Naive Bayes\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.5992982456140351\n",
      "TR (F1 score): 0.0\n",
      "AG (F1 score): 0.3397169025811823\n",
      "macro-averaged (F1 score): 0.3130050493984058 \n",
      "\n",
      "\n",
      "Model: Naive Bayes\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.6422594142259413\n",
      "TR (F1 score): 0.5991561181434599\n",
      "AG (F1 score): 0.43163538873994634\n",
      "macro-averaged (F1 score): 0.5576836403697826 \n",
      "\n",
      "\n",
      "Model: Naive Bayes\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.6572295247724974\n",
      "TR (F1 score): 0.5906542056074766\n",
      "AG (F1 score): 0.425\n",
      "macro-averaged (F1 score): 0.557627910126658 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # create a Naive Bayes classifier object\n",
    "  nb = GaussianNB()\n",
    "  # create a multioutput classifier object\n",
    "  multi_nb = MultiOutputClassifier(nb)\n",
    "  # fit the multioutput classifier on training data\n",
    "  if vector_type == 'Tfidf vectors':\n",
    "    multi_nb.fit(train_vectors[vector_type].toarray(), train_output)\n",
    "  else:\n",
    "    multi_nb.fit(train_vectors[vector_type], train_output)\n",
    "  # predict on test data\n",
    "  if vector_type == 'Tfidf vectors':\n",
    "    y_pred = multi_nb.predict(dev_vectors[vector_type].toarray())\n",
    "  else:\n",
    "    y_pred = multi_nb.predict(dev_vectors[vector_type])\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('Naive Bayes')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: Naive Bayes')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbours (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: KNN\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.6382978723404255\n",
      "TR (F1 score): 0.5406976744186046\n",
      "AG (F1 score): 0.28668941979522183\n",
      "macro-averaged (F1 score): 0.488561655518084 \n",
      "\n",
      "\n",
      "Model: KNN\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.5787159190853122\n",
      "TR (F1 score): 0.42916666666666664\n",
      "AG (F1 score): 0.21710526315789475\n",
      "macro-averaged (F1 score): 0.40832928296995785 \n",
      "\n",
      "\n",
      "Model: KNN\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.6142857142857142\n",
      "TR (F1 score): 0.4930747922437674\n",
      "AG (F1 score): 0.29447852760736193\n",
      "macro-averaged (F1 score): 0.4672796780456145 \n",
      "\n",
      "\n",
      "Model: KNN\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.6711560044893378\n",
      "TR (F1 score): 0.5604395604395604\n",
      "AG (F1 score): 0.38395415472779365\n",
      "macro-averaged (F1 score): 0.5385165732188973 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # create a KNN classifier object\n",
    "  knn = KNeighborsClassifier()\n",
    "  # create a multioutput classifier object\n",
    "  multi_knn = MultiOutputClassifier(knn)\n",
    "  # fit the multioutput classifier on training data\n",
    "  multi_knn.fit(train_vectors[vector_type], train_output)\n",
    "  # predict on test data\n",
    "  y_pred = multi_knn.predict(dev_vectors[vector_type])\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('KNN')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: KNN')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.5500 - val_loss: 0.6749\n",
      "Epoch 2/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.3323 - val_loss: 0.7832\n",
      "Epoch 3/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.2206 - val_loss: 0.9907\n",
      "Epoch 4/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.1449 - val_loss: 1.0964\n",
      "Epoch 5/40\n",
      "197/197 [==============================] - 11s 58ms/step - loss: 0.0901 - val_loss: 1.3723\n",
      "Epoch 6/40\n",
      "197/197 [==============================] - 11s 58ms/step - loss: 0.0551 - val_loss: 1.4377\n",
      "Epoch 7/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0384 - val_loss: 1.6654\n",
      "Epoch 8/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.0256 - val_loss: 1.8946\n",
      "Epoch 9/40\n",
      "197/197 [==============================] - 14s 69ms/step - loss: 0.0188 - val_loss: 2.0270\n",
      "Epoch 10/40\n",
      "197/197 [==============================] - 12s 62ms/step - loss: 0.0140 - val_loss: 1.9739\n",
      "Epoch 11/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0121 - val_loss: 2.1914\n",
      "Epoch 12/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.0097 - val_loss: 2.3812\n",
      "Epoch 13/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.0068 - val_loss: 2.4275\n",
      "Epoch 14/40\n",
      "197/197 [==============================] - 12s 62ms/step - loss: 0.0058 - val_loss: 2.6240\n",
      "Epoch 15/40\n",
      "197/197 [==============================] - 11s 57ms/step - loss: 0.0051 - val_loss: 2.4699\n",
      "Epoch 16/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.0047 - val_loss: 2.8031\n",
      "Epoch 17/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0032 - val_loss: 2.8696\n",
      "Epoch 18/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0031 - val_loss: 2.8839\n",
      "Epoch 19/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0033 - val_loss: 2.9119\n",
      "Epoch 20/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0030 - val_loss: 3.0201\n",
      "Epoch 21/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.0025 - val_loss: 3.1715\n",
      "Epoch 22/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 0.0019 - val_loss: 3.1764\n",
      "Epoch 23/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 0.0019 - val_loss: 3.3011\n",
      "Epoch 24/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0020 - val_loss: 3.1504\n",
      "Epoch 25/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 0.0017 - val_loss: 3.5405\n",
      "Epoch 26/40\n",
      "197/197 [==============================] - 12s 62ms/step - loss: 0.0023 - val_loss: 3.4969\n",
      "Epoch 27/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 0.0017 - val_loss: 3.4613\n",
      "Epoch 28/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0016 - val_loss: 3.5858\n",
      "Epoch 29/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 0.0015 - val_loss: 3.5243\n",
      "Epoch 30/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 9.8976e-04 - val_loss: 3.5204\n",
      "Epoch 31/40\n",
      "197/197 [==============================] - 12s 63ms/step - loss: 0.0010 - val_loss: 3.5935\n",
      "Epoch 32/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 8.5877e-04 - val_loss: 3.6851\n",
      "Epoch 33/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 0.0017 - val_loss: 3.7732\n",
      "Epoch 34/40\n",
      "197/197 [==============================] - 12s 62ms/step - loss: 0.0010 - val_loss: 3.5947\n",
      "Epoch 35/40\n",
      "197/197 [==============================] - 11s 58ms/step - loss: 5.3495e-04 - val_loss: 4.1139\n",
      "Epoch 36/40\n",
      "197/197 [==============================] - 12s 60ms/step - loss: 6.8695e-04 - val_loss: 4.3785\n",
      "Epoch 37/40\n",
      "197/197 [==============================] - 11s 58ms/step - loss: 6.5781e-04 - val_loss: 4.3446\n",
      "Epoch 38/40\n",
      "197/197 [==============================] - 12s 59ms/step - loss: 4.5952e-04 - val_loss: 4.3976\n",
      "Epoch 39/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 5.7437e-04 - val_loss: 4.3629\n",
      "Epoch 40/40\n",
      "197/197 [==============================] - 12s 61ms/step - loss: 6.1796e-04 - val_loss: 4.2613\n",
      "32/32 [==============================] - 1s 38ms/step\n",
      "Model: Neural Network\n",
      "Vectors: Tfidf vectors\n",
      "HS (F1 score): 0.6678445229681979\n",
      "TR (F1 score): 0.548148148148148\n",
      "AG (F1 score): 0.3958762886597938\n",
      "macro-averaged (F1 score): 0.5372896532587133 \n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.5439 - val_loss: 0.5095\n",
      "Epoch 2/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.5004 - val_loss: 0.5124\n",
      "Epoch 3/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4909 - val_loss: 0.5919\n",
      "Epoch 4/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4806 - val_loss: 0.6075\n",
      "Epoch 5/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4716 - val_loss: 0.4414\n",
      "Epoch 6/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4687 - val_loss: 0.5259\n",
      "Epoch 7/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4612 - val_loss: 0.4745\n",
      "Epoch 8/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4579 - val_loss: 0.6379\n",
      "Epoch 9/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4535 - val_loss: 0.5362\n",
      "Epoch 10/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4499 - val_loss: 0.6222\n",
      "Epoch 11/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4439 - val_loss: 0.5693\n",
      "Epoch 12/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4433 - val_loss: 0.6398\n",
      "Epoch 13/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4363 - val_loss: 0.5461\n",
      "Epoch 14/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4331 - val_loss: 0.7105\n",
      "Epoch 15/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4296 - val_loss: 0.8855\n",
      "Epoch 16/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4254 - val_loss: 0.8329\n",
      "Epoch 17/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4222 - val_loss: 0.6276\n",
      "Epoch 18/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4215 - val_loss: 0.5274\n",
      "Epoch 19/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4187 - val_loss: 0.5473\n",
      "Epoch 20/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4189 - val_loss: 0.6354\n",
      "Epoch 21/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4127 - val_loss: 0.5842\n",
      "Epoch 22/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4168 - val_loss: 0.7455\n",
      "Epoch 23/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4118 - val_loss: 0.7460\n",
      "Epoch 24/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4141 - val_loss: 0.5532\n",
      "Epoch 25/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4103 - val_loss: 0.6523\n",
      "Epoch 26/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4102 - val_loss: 0.5992\n",
      "Epoch 27/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4109 - val_loss: 0.6897\n",
      "Epoch 28/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4113 - val_loss: 0.6880\n",
      "Epoch 29/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4097 - val_loss: 0.7482\n",
      "Epoch 30/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4039 - val_loss: 0.6970\n",
      "Epoch 31/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4067 - val_loss: 0.5903\n",
      "Epoch 32/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4104 - val_loss: 0.6121\n",
      "Epoch 33/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.6737\n",
      "Epoch 34/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4068 - val_loss: 0.7262\n",
      "Epoch 35/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4063 - val_loss: 0.7770\n",
      "Epoch 36/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4042 - val_loss: 0.5295\n",
      "Epoch 37/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4075 - val_loss: 0.6232\n",
      "Epoch 38/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4048 - val_loss: 0.5899\n",
      "Epoch 39/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.6086\n",
      "Epoch 40/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4064 - val_loss: 0.7860\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Model: Neural Network\n",
      "Vectors: Word2Vec vectors\n",
      "HS (F1 score): 0.5988779803646564\n",
      "TR (F1 score): 0.3533697632058288\n",
      "AG (F1 score): 0.0\n",
      "macro-averaged (F1 score): 0.3174159145234951 \n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4806 - val_loss: 0.7522\n",
      "Epoch 2/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.4036 - val_loss: 0.7260\n",
      "Epoch 3/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3828 - val_loss: 0.8001\n",
      "Epoch 4/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3736 - val_loss: 0.8437\n",
      "Epoch 5/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.3587 - val_loss: 0.8225\n",
      "Epoch 6/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3547 - val_loss: 0.8270\n",
      "Epoch 7/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.3438 - val_loss: 0.8251\n",
      "Epoch 8/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3368 - val_loss: 0.8548\n",
      "Epoch 9/40\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.3288 - val_loss: 0.9793\n",
      "Epoch 10/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3218 - val_loss: 1.0435\n",
      "Epoch 11/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3184 - val_loss: 0.8468\n",
      "Epoch 12/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3100 - val_loss: 1.0131\n",
      "Epoch 13/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.3006 - val_loss: 1.0332\n",
      "Epoch 14/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2933 - val_loss: 1.0422\n",
      "Epoch 15/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2915 - val_loss: 1.0154\n",
      "Epoch 16/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2815 - val_loss: 1.0795\n",
      "Epoch 17/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2724 - val_loss: 1.1543\n",
      "Epoch 18/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2678 - val_loss: 1.0018\n",
      "Epoch 19/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2588 - val_loss: 1.2189\n",
      "Epoch 20/40\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.2554 - val_loss: 1.2631\n",
      "Epoch 21/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2500 - val_loss: 1.0884\n",
      "Epoch 22/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2427 - val_loss: 1.3199\n",
      "Epoch 23/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2372 - val_loss: 1.0545\n",
      "Epoch 24/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2331 - val_loss: 1.4423\n",
      "Epoch 25/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2260 - val_loss: 1.2946\n",
      "Epoch 26/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2199 - val_loss: 1.2753\n",
      "Epoch 27/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2136 - val_loss: 1.5090\n",
      "Epoch 28/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.2108 - val_loss: 1.3396\n",
      "Epoch 29/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 1.4319\n",
      "Epoch 30/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 1.3126\n",
      "Epoch 31/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1985 - val_loss: 1.4671\n",
      "Epoch 32/40\n",
      "197/197 [==============================] - 1s 4ms/step - loss: 0.1911 - val_loss: 1.6085\n",
      "Epoch 33/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1883 - val_loss: 1.5395\n",
      "Epoch 34/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1852 - val_loss: 1.5004\n",
      "Epoch 35/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.1816 - val_loss: 1.5296\n",
      "Epoch 36/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1793 - val_loss: 1.6956\n",
      "Epoch 37/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1744 - val_loss: 1.7396\n",
      "Epoch 38/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1770 - val_loss: 1.6633\n",
      "Epoch 39/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1690 - val_loss: 1.6266\n",
      "Epoch 40/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.1629 - val_loss: 1.5978\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Model: Neural Network\n",
      "Vectors: GloVe vectors\n",
      "HS (F1 score): 0.6524685382381413\n",
      "TR (F1 score): 0.5297029702970297\n",
      "AG (F1 score): 0.390625\n",
      "macro-averaged (F1 score): 0.524265502845057 \n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "197/197 [==============================] - 3s 16ms/step - loss: 0.4490 - val_loss: 0.7124\n",
      "Epoch 2/40\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.3925 - val_loss: 0.7283\n",
      "Epoch 3/40\n",
      "197/197 [==============================] - 2s 11ms/step - loss: 0.3784 - val_loss: 0.9530\n",
      "Epoch 4/40\n",
      "197/197 [==============================] - 3s 14ms/step - loss: 0.3659 - val_loss: 0.8187\n",
      "Epoch 5/40\n",
      "197/197 [==============================] - 2s 13ms/step - loss: 0.3582 - val_loss: 0.7268\n",
      "Epoch 6/40\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 0.3502 - val_loss: 0.9592\n",
      "Epoch 7/40\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.3428 - val_loss: 0.9949\n",
      "Epoch 8/40\n",
      "197/197 [==============================] - 2s 13ms/step - loss: 0.3375 - val_loss: 0.9173\n",
      "Epoch 9/40\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 0.3352 - val_loss: 1.0235\n",
      "Epoch 10/40\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.3297 - val_loss: 1.0112\n",
      "Epoch 11/40\n",
      "197/197 [==============================] - 3s 14ms/step - loss: 0.3198 - val_loss: 0.9967\n",
      "Epoch 12/40\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.3147 - val_loss: 0.8967\n",
      "Epoch 13/40\n",
      "197/197 [==============================] - 2s 11ms/step - loss: 0.3112 - val_loss: 0.9137\n",
      "Epoch 14/40\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.3068 - val_loss: 1.1725\n",
      "Epoch 15/40\n",
      "197/197 [==============================] - 3s 14ms/step - loss: 0.3029 - val_loss: 0.8036\n",
      "Epoch 16/40\n",
      "197/197 [==============================] - 3s 14ms/step - loss: 0.2967 - val_loss: 1.1485\n",
      "Epoch 17/40\n",
      "197/197 [==============================] - 3s 16ms/step - loss: 0.2931 - val_loss: 1.2615\n",
      "Epoch 18/40\n",
      "197/197 [==============================] - 3s 13ms/step - loss: 0.2888 - val_loss: 1.0311\n",
      "Epoch 19/40\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 0.2810 - val_loss: 1.1232\n",
      "Epoch 20/40\n",
      "197/197 [==============================] - 3s 16ms/step - loss: 0.2776 - val_loss: 1.2578\n",
      "Epoch 21/40\n",
      "197/197 [==============================] - 3s 16ms/step - loss: 0.2726 - val_loss: 1.0175\n",
      "Epoch 22/40\n",
      "197/197 [==============================] - 2s 12ms/step - loss: 0.2703 - val_loss: 1.2391\n",
      "Epoch 23/40\n",
      "197/197 [==============================] - 3s 15ms/step - loss: 0.2615 - val_loss: 1.1903\n",
      "Epoch 24/40\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.2591 - val_loss: 1.2919\n",
      "Epoch 25/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2521 - val_loss: 1.3524\n",
      "Epoch 26/40\n",
      "197/197 [==============================] - 1s 6ms/step - loss: 0.2477 - val_loss: 1.7368\n",
      "Epoch 27/40\n",
      "197/197 [==============================] - 1s 5ms/step - loss: 0.2419 - val_loss: 1.4169\n",
      "Epoch 28/40\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.2381 - val_loss: 1.3529\n",
      "Epoch 29/40\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.2375 - val_loss: 1.2778\n",
      "Epoch 30/40\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.2333 - val_loss: 1.4606\n",
      "Epoch 31/40\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.2290 - val_loss: 1.3933\n",
      "Epoch 32/40\n",
      "197/197 [==============================] - 2s 9ms/step - loss: 0.2235 - val_loss: 1.4876\n",
      "Epoch 33/40\n",
      "197/197 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 1.5191\n",
      "Epoch 34/40\n",
      "197/197 [==============================] - 2s 8ms/step - loss: 0.2121 - val_loss: 1.6343\n",
      "Epoch 35/40\n",
      "197/197 [==============================] - 2s 10ms/step - loss: 0.2104 - val_loss: 1.6698\n",
      "Epoch 36/40\n",
      "197/197 [==============================] - 2s 11ms/step - loss: 0.2079 - val_loss: 1.5609\n",
      "Epoch 37/40\n",
      "197/197 [==============================] - 3s 13ms/step - loss: 0.2040 - val_loss: 1.7676\n",
      "Epoch 38/40\n",
      "197/197 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 1.6425\n",
      "Epoch 39/40\n",
      "197/197 [==============================] - 4s 18ms/step - loss: 0.1981 - val_loss: 1.7097\n",
      "Epoch 40/40\n",
      "197/197 [==============================] - 3s 14ms/step - loss: 0.1921 - val_loss: 1.7333\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Model: Neural Network\n",
      "Vectors: BERT vectors\n",
      "HS (F1 score): 0.6752058554437329\n",
      "TR (F1 score): 0.6388308977035491\n",
      "AG (F1 score): 0.4089887640449439\n",
      "macro-averaged (F1 score): 0.5743418390640753 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_type in vector_types:\n",
    "  # define model architecture\n",
    "  if vector_type == 'Tfidf vectors':\n",
    "    input_layer = Input(shape=len(train_vectors[vector_type].toarray()[0]))\n",
    "  else:\n",
    "    input_layer = Input(shape=len(train_vectors[vector_type][0]))\n",
    "  dense_layer1 = Dense(64, activation='relu')(input_layer)\n",
    "  dropout_layer1 = Dropout(0.2)(dense_layer1)\n",
    "  dense_layer2 = Dense(32, activation='relu')(dropout_layer1)\n",
    "  dropout_layer2 = Dropout(0.2)(dense_layer2)\n",
    "  output_layer = Dense(3, activation='sigmoid')(dropout_layer2)\n",
    "\n",
    "  model = Model(inputs=input_layer, outputs=output_layer)\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "  # define ideal batch size\n",
    "  batch_size = 32\n",
    "  # train the model\n",
    "  if vector_type == 'Tfidf vectors':\n",
    "    history = model.fit(train_vectors[vector_type].toarray(), train_output, epochs=40, batch_size=batch_size, validation_split=0.3)\n",
    "  else:\n",
    "    history = model.fit(train_vectors[vector_type], train_output, epochs=40, batch_size=batch_size, validation_split=0.3)\n",
    "  # make predictions on test data\n",
    "  if vector_type == 'Tfidf vectors':\n",
    "    predictions = model.predict(dev_vectors[vector_type].toarray(), batch_size=batch_size)\n",
    "  else:\n",
    "    predictions = model.predict(dev_vectors[vector_type], batch_size=batch_size)\n",
    "  # round the predictions to 0 or 1\n",
    "  rounded_predictions = np.round(predictions)\n",
    "  # classification report\n",
    "  report = classification_report(dev_output, rounded_predictions, target_names=labels, zero_division=0, output_dict=True)\n",
    "  # storing and printing the results\n",
    "  evaluation_results['model type'].append('Neural Network')\n",
    "  evaluation_results['vector type'].append(vector_type)\n",
    "  evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "  evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "  evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "  evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "  print('Model: Neural Network')\n",
    "  print(f'Vectors: {vector_type}')\n",
    "  print('HS (F1 score):', report['HS']['f1-score'])\n",
    "  print('TR (F1 score):', report['TR']['f1-score'])\n",
    "  print('AG (F1 score):', report['AG']['f1-score'])\n",
    "  print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "197/197 [==============================] - 42s 207ms/step - loss: 0.4416 - val_loss: 0.8906\n",
      "Epoch 2/30\n",
      "197/197 [==============================] - 42s 211ms/step - loss: 0.2935 - val_loss: 1.1338\n",
      "Epoch 3/30\n",
      "197/197 [==============================] - 42s 212ms/step - loss: 0.2100 - val_loss: 1.1111\n",
      "Epoch 4/30\n",
      "197/197 [==============================] - 51s 258ms/step - loss: 0.1429 - val_loss: 1.5419\n",
      "Epoch 5/30\n",
      "197/197 [==============================] - 46s 232ms/step - loss: 0.0960 - val_loss: 1.6267\n",
      "Epoch 6/30\n",
      "197/197 [==============================] - 49s 244ms/step - loss: 0.0689 - val_loss: 1.6361\n",
      "Epoch 7/30\n",
      "197/197 [==============================] - 43s 219ms/step - loss: 0.0493 - val_loss: 1.9480\n",
      "Epoch 8/30\n",
      "197/197 [==============================] - 41s 206ms/step - loss: 0.0426 - val_loss: 2.0216\n",
      "Epoch 9/30\n",
      "197/197 [==============================] - 47s 237ms/step - loss: 0.0301 - val_loss: 2.3072\n",
      "Epoch 10/30\n",
      "197/197 [==============================] - 48s 245ms/step - loss: 0.0247 - val_loss: 2.1937\n",
      "Epoch 11/30\n",
      "197/197 [==============================] - 41s 210ms/step - loss: 0.0194 - val_loss: 2.2989\n",
      "Epoch 12/30\n",
      "197/197 [==============================] - 46s 234ms/step - loss: 0.0157 - val_loss: 2.6294\n",
      "Epoch 13/30\n",
      "197/197 [==============================] - 41s 206ms/step - loss: 0.0149 - val_loss: 2.6289\n",
      "Epoch 14/30\n",
      "197/197 [==============================] - 39s 200ms/step - loss: 0.0115 - val_loss: 2.9408\n",
      "Epoch 15/30\n",
      "197/197 [==============================] - 44s 224ms/step - loss: 0.0078 - val_loss: 2.9667\n",
      "Epoch 16/30\n",
      "197/197 [==============================] - 44s 222ms/step - loss: 0.0060 - val_loss: 3.2017\n",
      "Epoch 17/30\n",
      "197/197 [==============================] - 42s 214ms/step - loss: 0.0076 - val_loss: 3.2679\n",
      "Epoch 18/30\n",
      "197/197 [==============================] - 35s 177ms/step - loss: 0.0137 - val_loss: 2.9239\n",
      "Epoch 19/30\n",
      "197/197 [==============================] - 37s 187ms/step - loss: 0.0105 - val_loss: 3.2351\n",
      "Epoch 20/30\n",
      "197/197 [==============================] - 45s 228ms/step - loss: 0.0088 - val_loss: 3.2993\n",
      "Epoch 21/30\n",
      "197/197 [==============================] - 52s 263ms/step - loss: 0.0076 - val_loss: 3.4262\n",
      "Epoch 22/30\n",
      "197/197 [==============================] - 48s 245ms/step - loss: 0.0082 - val_loss: 3.1270\n",
      "Epoch 23/30\n",
      "197/197 [==============================] - 42s 213ms/step - loss: 0.0028 - val_loss: 3.5945\n",
      "Epoch 24/30\n",
      "197/197 [==============================] - 41s 206ms/step - loss: 0.0016 - val_loss: 4.0644\n",
      "Epoch 25/30\n",
      "197/197 [==============================] - 39s 199ms/step - loss: 0.0010 - val_loss: 3.9659\n",
      "Epoch 26/30\n",
      "197/197 [==============================] - 40s 204ms/step - loss: 0.0010 - val_loss: 4.1997\n",
      "Epoch 27/30\n",
      "197/197 [==============================] - 42s 213ms/step - loss: 8.3899e-04 - val_loss: 4.1997\n",
      "Epoch 28/30\n",
      "197/197 [==============================] - 40s 205ms/step - loss: 5.6933e-04 - val_loss: 4.4774\n",
      "Epoch 29/30\n",
      "197/197 [==============================] - 45s 230ms/step - loss: 5.2866e-04 - val_loss: 4.5389\n",
      "Epoch 30/30\n",
      "197/197 [==============================] - 49s 248ms/step - loss: 4.3673e-04 - val_loss: 4.7169\n",
      "32/32 [==============================] - 2s 48ms/step\n",
      "Model: LSTM\n",
      "HS (F1 score): 0.672\n",
      "TR (F1 score): 0.5649484536082474\n",
      "AG (F1 score): 0.4232365145228216\n",
      "macro-averaged (F1 score): 0.553394989377023 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum number of words to consider and the maximum sequence length\n",
    "max_words = 10000\n",
    "maxlen = 100\n",
    "\n",
    "# Create a tokenizer and fit it to the training text data\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "\n",
    "# Convert the training and dev text data to sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_data['text'])\n",
    "\n",
    "# Pad the sequences to ensure they all have the same length\n",
    "train_lstm = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "dev_lstm = pad_sequences(dev_sequences, maxlen=maxlen)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_lstm, train_output, epochs=30, batch_size=32, validation_split=0.3)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "y_pred = model.predict(dev_lstm)\n",
    "\n",
    "# Convert the predicted probabilities to binary labels\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# classification report\n",
    "report = classification_report(dev_output, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "\n",
    "print('Model: LSTM')\n",
    "print('HS (F1 score):', report['HS']['f1-score'])\n",
    "print('TR (F1 score):', report['TR']['f1-score'])\n",
    "print('AG (F1 score):', report['AG']['f1-score'])\n",
    "print('macro-averaged (F1 score):', report['macro avg']['f1-score'], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the results\n",
    "evaluation_results['model type'].append('LSTM')\n",
    "evaluation_results['vector type'].append('')\n",
    "evaluation_results['HS F1'].append(report['HS']['f1-score'])\n",
    "evaluation_results['TR F1'].append(report['TR']['f1-score'])\n",
    "evaluation_results['AG F1'].append(report['AG']['f1-score'])\n",
    "evaluation_results['macro-averaged F1'].append(report['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model type</th>\n",
       "      <th>vector type</th>\n",
       "      <th>HS F1</th>\n",
       "      <th>TR F1</th>\n",
       "      <th>AG F1</th>\n",
       "      <th>macro-averaged F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.699647</td>\n",
       "      <td>0.538012</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>0.535238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.645465</td>\n",
       "      <td>0.436923</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.436613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.679724</td>\n",
       "      <td>0.597826</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.566201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.619816</td>\n",
       "      <td>0.567237</td>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.539203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.546547</td>\n",
       "      <td>0.292994</td>\n",
       "      <td>0.254118</td>\n",
       "      <td>0.364553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.561135</td>\n",
       "      <td>0.426735</td>\n",
       "      <td>0.287105</td>\n",
       "      <td>0.424992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.579186</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.350467</td>\n",
       "      <td>0.449188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.696386</td>\n",
       "      <td>0.525994</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.520136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.574241</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.339781</td>\n",
       "      <td>0.419901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.646778</td>\n",
       "      <td>0.357388</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.358746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.645631</td>\n",
       "      <td>0.374558</td>\n",
       "      <td>0.182573</td>\n",
       "      <td>0.400921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.713781</td>\n",
       "      <td>0.548571</td>\n",
       "      <td>0.339350</td>\n",
       "      <td>0.533901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.605839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.686775</td>\n",
       "      <td>0.446602</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.411126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.697954</td>\n",
       "      <td>0.463492</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.453264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.626024</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.335640</td>\n",
       "      <td>0.465482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.599298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339717</td>\n",
       "      <td>0.313005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.642259</td>\n",
       "      <td>0.599156</td>\n",
       "      <td>0.431635</td>\n",
       "      <td>0.557684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.657230</td>\n",
       "      <td>0.590654</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.557628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.286689</td>\n",
       "      <td>0.488562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.578716</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.217105</td>\n",
       "      <td>0.408329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>KNN</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.493075</td>\n",
       "      <td>0.294479</td>\n",
       "      <td>0.467280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>KNN</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.671156</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.383954</td>\n",
       "      <td>0.538517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>Tfidf vectors</td>\n",
       "      <td>0.667845</td>\n",
       "      <td>0.548148</td>\n",
       "      <td>0.395876</td>\n",
       "      <td>0.537290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>Word2Vec vectors</td>\n",
       "      <td>0.598878</td>\n",
       "      <td>0.353370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>GloVe vectors</td>\n",
       "      <td>0.652469</td>\n",
       "      <td>0.529703</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.524266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>BERT vectors</td>\n",
       "      <td>0.675206</td>\n",
       "      <td>0.638831</td>\n",
       "      <td>0.408989</td>\n",
       "      <td>0.574342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LSTM</td>\n",
       "      <td></td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.564948</td>\n",
       "      <td>0.423237</td>\n",
       "      <td>0.553395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model type       vector type     HS F1     TR F1     AG F1  \\\n",
       "0      Logistic Regression     Tfidf vectors  0.699647  0.538012  0.368056   \n",
       "1      Logistic Regression  Word2Vec vectors  0.277778  0.365000  0.000000   \n",
       "2      Logistic Regression     GloVe vectors  0.645465  0.436923  0.227451   \n",
       "3      Logistic Regression      BERT vectors  0.679724  0.597826  0.421053   \n",
       "4            Decision Tree     Tfidf vectors  0.619816  0.567237  0.430556   \n",
       "5            Decision Tree  Word2Vec vectors  0.546547  0.292994  0.254118   \n",
       "6            Decision Tree     GloVe vectors  0.561135  0.426735  0.287105   \n",
       "7            Decision Tree      BERT vectors  0.579186  0.417910  0.350467   \n",
       "8            Random Forest     Tfidf vectors  0.696386  0.525994  0.338028   \n",
       "9            Random Forest  Word2Vec vectors  0.574241  0.345679  0.339781   \n",
       "10           Random Forest     GloVe vectors  0.646778  0.357388  0.072072   \n",
       "11           Random Forest      BERT vectors  0.645631  0.374558  0.182573   \n",
       "12  Support Vector Machine     Tfidf vectors  0.713781  0.548571  0.339350   \n",
       "13  Support Vector Machine  Word2Vec vectors  0.605839  0.000000  0.000000   \n",
       "14  Support Vector Machine     GloVe vectors  0.686775  0.446602  0.100000   \n",
       "15  Support Vector Machine      BERT vectors  0.697954  0.463492  0.198347   \n",
       "16             Naive Bayes     Tfidf vectors  0.626024  0.434783  0.335640   \n",
       "17             Naive Bayes  Word2Vec vectors  0.599298  0.000000  0.339717   \n",
       "18             Naive Bayes     GloVe vectors  0.642259  0.599156  0.431635   \n",
       "19             Naive Bayes      BERT vectors  0.657230  0.590654  0.425000   \n",
       "20                     KNN     Tfidf vectors  0.638298  0.540698  0.286689   \n",
       "21                     KNN  Word2Vec vectors  0.578716  0.429167  0.217105   \n",
       "22                     KNN     GloVe vectors  0.614286  0.493075  0.294479   \n",
       "23                     KNN      BERT vectors  0.671156  0.560440  0.383954   \n",
       "24          Neural Network     Tfidf vectors  0.667845  0.548148  0.395876   \n",
       "25          Neural Network  Word2Vec vectors  0.598878  0.353370  0.000000   \n",
       "26          Neural Network     GloVe vectors  0.652469  0.529703  0.390625   \n",
       "27          Neural Network      BERT vectors  0.675206  0.638831  0.408989   \n",
       "28                    LSTM                    0.672000  0.564948  0.423237   \n",
       "\n",
       "    macro-averaged F1  \n",
       "0            0.535238  \n",
       "1            0.214259  \n",
       "2            0.436613  \n",
       "3            0.566201  \n",
       "4            0.539203  \n",
       "5            0.364553  \n",
       "6            0.424992  \n",
       "7            0.449188  \n",
       "8            0.520136  \n",
       "9            0.419901  \n",
       "10           0.358746  \n",
       "11           0.400921  \n",
       "12           0.533901  \n",
       "13           0.201946  \n",
       "14           0.411126  \n",
       "15           0.453264  \n",
       "16           0.465482  \n",
       "17           0.313005  \n",
       "18           0.557684  \n",
       "19           0.557628  \n",
       "20           0.488562  \n",
       "21           0.408329  \n",
       "22           0.467280  \n",
       "23           0.538517  \n",
       "24           0.537290  \n",
       "25           0.317416  \n",
       "26           0.524266  \n",
       "27           0.574342  \n",
       "28           0.553395  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(evaluation_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the results\n",
    "with open('results.pkl', 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "with open('results.pkl', 'rb') as f:\n",
    "    diya = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model type': ['Logistic Regression',\n",
       "  'Logistic Regression',\n",
       "  'Logistic Regression',\n",
       "  'Logistic Regression',\n",
       "  'Decision Tree',\n",
       "  'Decision Tree',\n",
       "  'Decision Tree',\n",
       "  'Decision Tree',\n",
       "  'Random Forest',\n",
       "  'Random Forest',\n",
       "  'Random Forest',\n",
       "  'Random Forest',\n",
       "  'Support Vector Machine',\n",
       "  'Support Vector Machine',\n",
       "  'Support Vector Machine',\n",
       "  'Support Vector Machine',\n",
       "  'Naive Bayes',\n",
       "  'Naive Bayes',\n",
       "  'Naive Bayes',\n",
       "  'Naive Bayes',\n",
       "  'KNN',\n",
       "  'KNN',\n",
       "  'KNN',\n",
       "  'KNN',\n",
       "  'Neural Network',\n",
       "  'Neural Network',\n",
       "  'Neural Network',\n",
       "  'Neural Network',\n",
       "  'LSTM'],\n",
       " 'vector type': ['Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  'Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  'Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  'Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  'Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  'Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  'Tfidf vectors',\n",
       "  'Word2Vec vectors',\n",
       "  'GloVe vectors',\n",
       "  'BERT vectors',\n",
       "  ''],\n",
       " 'HS F1': [0.6996466431095407,\n",
       "  0.2777777777777778,\n",
       "  0.645465253239105,\n",
       "  0.6797235023041475,\n",
       "  0.6198156682027651,\n",
       "  0.5465465465465464,\n",
       "  0.5611353711790393,\n",
       "  0.579185520361991,\n",
       "  0.6963855421686748,\n",
       "  0.5742411812961443,\n",
       "  0.6467780429594272,\n",
       "  0.6456310679611651,\n",
       "  0.7137809187279152,\n",
       "  0.6058394160583942,\n",
       "  0.6867749419953597,\n",
       "  0.6979542719614921,\n",
       "  0.6260236578707916,\n",
       "  0.5992982456140351,\n",
       "  0.6422594142259413,\n",
       "  0.6572295247724974,\n",
       "  0.6382978723404255,\n",
       "  0.5787159190853122,\n",
       "  0.6142857142857142,\n",
       "  0.6711560044893378,\n",
       "  0.6678445229681979,\n",
       "  0.5988779803646564,\n",
       "  0.6524685382381413,\n",
       "  0.6752058554437329,\n",
       "  0.672],\n",
       " 'TR F1': [0.5380116959064327,\n",
       "  0.36500000000000005,\n",
       "  0.43692307692307686,\n",
       "  0.5978260869565217,\n",
       "  0.567237163814181,\n",
       "  0.2929936305732484,\n",
       "  0.4267352185089974,\n",
       "  0.41791044776119396,\n",
       "  0.5259938837920489,\n",
       "  0.345679012345679,\n",
       "  0.35738831615120276,\n",
       "  0.3745583038869258,\n",
       "  0.5485714285714286,\n",
       "  0.0,\n",
       "  0.4466019417475728,\n",
       "  0.46349206349206346,\n",
       "  0.43478260869565216,\n",
       "  0.0,\n",
       "  0.5991561181434599,\n",
       "  0.5906542056074766,\n",
       "  0.5406976744186046,\n",
       "  0.42916666666666664,\n",
       "  0.4930747922437674,\n",
       "  0.5604395604395604,\n",
       "  0.548148148148148,\n",
       "  0.3533697632058288,\n",
       "  0.5297029702970297,\n",
       "  0.6388308977035491,\n",
       "  0.5649484536082474],\n",
       " 'AG F1': [0.3680555555555556,\n",
       "  0.0,\n",
       "  0.22745098039215686,\n",
       "  0.4210526315789474,\n",
       "  0.4305555555555555,\n",
       "  0.2541176470588235,\n",
       "  0.2871046228710462,\n",
       "  0.35046728971962615,\n",
       "  0.33802816901408456,\n",
       "  0.33978132884777124,\n",
       "  0.07207207207207207,\n",
       "  0.18257261410788386,\n",
       "  0.33935018050541516,\n",
       "  0.0,\n",
       "  0.09999999999999999,\n",
       "  0.19834710743801653,\n",
       "  0.3356401384083045,\n",
       "  0.3397169025811823,\n",
       "  0.43163538873994634,\n",
       "  0.425,\n",
       "  0.28668941979522183,\n",
       "  0.21710526315789475,\n",
       "  0.29447852760736193,\n",
       "  0.38395415472779365,\n",
       "  0.3958762886597938,\n",
       "  0.0,\n",
       "  0.390625,\n",
       "  0.4089887640449439,\n",
       "  0.4232365145228216],\n",
       " 'macro-averaged F1': [0.5352379648571763,\n",
       "  0.2142592592592593,\n",
       "  0.43661310351811294,\n",
       "  0.5662007402798722,\n",
       "  0.5392027958575005,\n",
       "  0.36455260805953943,\n",
       "  0.4249917375196943,\n",
       "  0.4491877526142704,\n",
       "  0.5201358649916027,\n",
       "  0.41990050749653146,\n",
       "  0.35874614372756736,\n",
       "  0.4009206619853249,\n",
       "  0.5339008426015863,\n",
       "  0.20194647201946472,\n",
       "  0.4111256279143109,\n",
       "  0.45326448096385735,\n",
       "  0.46548213499158275,\n",
       "  0.3130050493984058,\n",
       "  0.5576836403697826,\n",
       "  0.557627910126658,\n",
       "  0.488561655518084,\n",
       "  0.40832928296995785,\n",
       "  0.4672796780456145,\n",
       "  0.5385165732188973,\n",
       "  0.5372896532587133,\n",
       "  0.3174159145234951,\n",
       "  0.524265502845057,\n",
       "  0.5743418390640753,\n",
       "  0.553394989377023]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
